{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exploring Altruism in Social Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model shows how altruistic behaviour emerges in social networks from an entirely selfish group. The analysis is conducted by simulating the ultimatum game on a Barabasi-Albert network with different degree of preferential attachment. While previous work studied the emergence of altruistic behaviour in a well mixed population, this paper will study how social networks can limit the arise of altruism. Agents in the model are empathic and memoryless and the strategic update rule is natural selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work is published here https://www.researchgate.net/publication/269400895_Exploring_Altruism_in_Social_Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model assumes that all agents $N$ are memoryless and empathic. each agent has a threshold $t_i$ which is an integer number indicates the amount of money the agent is willing to accept in the game, where $1 \\le t_i \\le M$. $M$ is total amount available each game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Initialization(N): \n",
    "    agentsdummy = np.zeros((2,N),dtype=int) #threshold, capital\n",
    "    agentsdummy[0,:1000:1]= 1 \n",
    "    agents = agentsdummy.T\n",
    "    return agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network construction Barabsi-Alberts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NetworkConstruction(N,agents,degree):\n",
    "    network = []\n",
    "    G = nx.barabasi_albert_graph(N,degree)\n",
    "    for i in xrange(N): # to group nodes to two groups selfish = 1, altruist =2\n",
    "        if agents[i][0] > 1:\n",
    "            G.add_node(i, Group=2)\n",
    "        else:\n",
    "             G.add_node(i, Group=1) \n",
    "    network = nx.to_dict_of_lists(G) #will use it to adjacent nodes later \n",
    "    return network,G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get connected nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighborhood(agent,network): \n",
    "    availablePlayers = network[agent]\n",
    "    return availablePlayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose players. In the previous model any player can be proposer and play with any other player as responder. Here any player can be chosen as proposer but only players in the proposer neighbourhood can be chosen as a responder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ChoosePlayers(agents, N, network): #choose the players, two agents each game like paper 1\n",
    "    currProposer = randint(1, N) \n",
    "    availablePlayers = neighborhood(currProposer,network) # who she can play with\n",
    "    currResponder = choice(availablePlayers)\n",
    "    return currProposer,currResponder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since players are empathic then the proposed amount will be equal to the proposer threshold $t_p$, so if the proposer threshold $t_p$ greater than the responder $t_r$ the offer will be accepted. Both players fitnesses will be updated as follow:\n",
    "\n",
    "$$\\eta_p = M - t_p$$\n",
    "$$\\eta_r = t_p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TheGame(agents, currProposer, currResponder, M): #capital update following the game rules\n",
    "    if agents[currProposer][0] >= agents[currResponder][0]: #assuming empathetic agents, where my threshold = proposal amount\n",
    "        agents[currProposer][1] += (M - agents[currProposer][0])\n",
    "        agents[currResponder][1] += agents[currProposer][0]\n",
    "    return agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After $S$ games, for each agent the agent with the maximum fitness in the neighborhood will substitute the agent with minimum fitness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Selection(agents): \n",
    "    agentsdummy = agents.T\n",
    "    possiblereplicaAgent = np.where(agentsdummy[1] == (max(agentsdummy[1])))\n",
    "    replicaAgentIndex = choice(possiblereplicaAgent[0])\n",
    "    possibleremovedAgent = np.where(agentsdummy[1] == (min(agentsdummy[1])))\n",
    "    removedAgentIndex = choice(possibleremovedAgent[0])\n",
    "    agents[removedAgentIndex][0] = agents[replicaAgentIndex][0]\n",
    "    agents[removedAgentIndex][1] = agents[replicaAgentIndex][1]\n",
    "    return agents, replicaAgentIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With probability $1/3$ the duplicated agent acceptance threshold will change $\\pm 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Mutation(agents,replicaAgentIndex,M):\n",
    "    mutationProb = random()\n",
    "    if mutationProb > 0.666666667  and agents[replicaAgentIndex][0] < M:\n",
    "        agents[replicaAgentIndex][0]= agents[replicaAgentIndex][0]+1\n",
    "    if mutationProb < 0.333333333 and agents[replicaAgentIndex][0] > 1:\n",
    "        agents[replicaAgentIndex][0]= agents[replicaAgentIndex][0]-1\n",
    "       \n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Run(N,M, s, numOfGames,degree):\n",
    "    clock = 1\n",
    "    agents = Initialization(N)\n",
    "    network,G = NetworkConstruction(N,agents,degree)\n",
    "\n",
    "    while(clock != numOfGames):\n",
    "        currProposer,currResponder = ChoosePlayers(agents, N, network)\n",
    "        agents = TheGame(agents, currProposer, currResponder, M)\n",
    "        if np.fmod(clock,s) == 0:\n",
    "            agents,replicaAgentIndex = Selection(agents)\n",
    "            agents = Mutation(agents,replicaAgentIndex,M)\n",
    "        clock = clock+1\n",
    "    frequency =  Counter(agents.T[0])\n",
    "    averageThreshold = sum(agents.T[0])/N\n",
    "    return averageThreshold, frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model here called only once (to show resultes fast) in order to get accurate stastistical analysis simulations runs were 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import networkx as nx\n",
    "from random import *\n",
    "from collections import Counter\n",
    "import pylab as plt\n",
    "\n",
    "i = 0\n",
    "averageThresholdAcrossRuns = []\n",
    "FreqsAcrossRuns =[]\n",
    "N = 1000\n",
    "degree = 10\n",
    "M = 100\n",
    "s = 10000\n",
    "numOfGames = 1\n",
    "averageThreshold, frequency = Run(N, M, s, numOfGames,degree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
